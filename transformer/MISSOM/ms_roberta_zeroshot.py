# -*- coding: utf-8 -*-
"""ms_roberta_zeroshot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ll7wcH0Ry46lfjZavICdMDwtIX0xhB1z
"""

import pandas as pd
import transformers
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn import metrics
from tqdm import tqdm
from collections import defaultdict
import numpy as np
import matplotlib.pyplot as plt
import json
import random

import warnings
warnings.filterwarnings('ignore')

torch.multiprocessing.set_sharing_strategy('file_system')

class Config:
    VALID_BATCH_SIZE = 4
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    TEST_DATA = "missom_test_869.csv"
    model_id = "roberta-large"
    pipeline = transformers.pipeline(
        "zero-shot-classification",
        model=model_id,
        model_kwargs={"torch_dtype": torch.float16},
        device=0 if torch.cuda.is_available() else -1
    )

class MinorityStressDataset(Dataset):
    def __init__(self, data_path, shuffle=False):
        self.data = pd.read_csv(data_path).fillna('none').reset_index(drop=True)
        if shuffle:
            self.data = self.data.sample(frac=1).reset_index(drop=True)  # Shuffle dataset
        self.text = self.data.text.values
        self.label = self.data.label.values

    def __len__(self):
        return len(self.text)

    def __getitem__(self, item):
        return {"text": self.text[item], "label": self.label[item]}

def classify_texts(texts):
    """Batch classification for efficiency."""
    responses = Config.pipeline(texts, candidate_labels=["minority stress", "no minority stress"])
    return [1 if r['labels'][0] == 'minority stress' else 0 for r in responses]

# Evaluation and ROC Curve
def save_roc_curve(y_true, test_probs, filename):
    """Save ROC curve as a plot and a text file."""
    fpr, tpr, thresholds = metrics.roc_curve(y_true, test_probs)
    auc_score = metrics.roc_auc_score(y_true, test_probs)
    data = np.column_stack((fpr, tpr, thresholds))
    header = "False Positive Rate (FPR)\tTrue Positive Rate (TPR)\tThresholds"
    np.savetxt(filename + '.txt', data, header=header, delimiter='\t')

    plt.figure()
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {auc_score:.4f})')
    plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.savefig(filename + '.png')
    plt.close()

def eval_fn(dataloader):
    """Evaluates the model using DataLoader and saves ROC curve."""
    fin_labels, fin_outputs = [], []

    for batch in tqdm(dataloader, desc="Evaluating"):
        texts, labels = batch["text"], batch["label"]
        predictions = classify_texts(texts)
        fin_labels.extend(labels)
        fin_outputs.extend(predictions)

    save_roc_curve(fin_labels, fin_outputs, 'ms_roc_curve_roberta_zero_shot')
    return fin_outputs, fin_labels

def run_multiple_evaluations(num_runs=5):
    """Runs multiple evaluations and aggregates results."""
    classification_reports = []

    for i in range(num_runs):
        print(f"Run {i+1}/{num_runs}")

        # Shuffle dataset before each run
        shuffled_data = MinorityStressDataset(Config.TEST_DATA, shuffle=True)
        shuffled_dataloader = DataLoader(shuffled_data, batch_size=Config.VALID_BATCH_SIZE, num_workers=4, shuffle=False)

        test_outputs, test_labels = eval_fn(shuffled_dataloader)
        report = metrics.classification_report(test_labels, test_outputs, output_dict=True)
        classification_reports.append(report)

        print(f"Classification Report for Run {i+1}:\n{metrics.classification_report(test_labels, test_outputs, digits=4)}")

    # Aggregate results
    averaged_report = defaultdict(dict)

    for key in classification_reports[0].keys():
        if isinstance(classification_reports[0][key], dict):  # If it's a dictionary (precision, recall, f1-score)
            for metric in classification_reports[0][key]:
                values = [report[key][metric] for report in classification_reports if isinstance(report[key], dict)]
                averaged_report[key][metric] = {
                    "mean": np.mean(values),
                    "std": np.std(values)
                }
        else:  # If it's a scalar value (like accuracy)
            values = [report[key] for report in classification_reports if isinstance(report[key], (int, float))]
            averaged_report[key] = {
                "mean": np.mean(values),
                "std": np.std(values)
            }

    print("===" * 50)
    print("Averaged Classification Report with Standard Deviations:")
    for label, metrics_data in averaged_report.items():
        if isinstance(metrics_data, dict):  # Ensure proper formatting
            print(f"Label: {label}")
            for metric, values in metrics_data.items():
                if isinstance(values, dict):
                    print(f"  {metric}: {values.get('mean', 0):.4f} ± {values.get('std', 0):.4f}")
        else:  # Handle scalar values
            print(f"  {label}: {metrics_data.get('mean', 0):.4f} ± {metrics_data.get('std', 0):.4f}")
    print("===" * 50)

    # Save report to JSON file
    with open('classification_reports.json', 'w') as f:
        json.dump(averaged_report, f, indent=4)

    return averaged_report

# Execute the multiple evaluation runs
if __name__ == "__main__":
    averaged_report = run_multiple_evaluations()